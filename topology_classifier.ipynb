{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc, confusion_matrix\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "from plot_imports import *\n",
    "from periodic_table import fetch_table, periodic_plot\n",
    "from utils import (get_species, load_data, get_roc, get_optimal_threshold, plot_roc, plot_precision_recall_fscore,\n",
    "                   get_element_results)\n",
    "\n",
    "seed = 14\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# format progress bar\n",
    "bar_format = '{l_bar}{bar:10}{r_bar}{bar:-10b}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/data_merged.csv'\n",
    "energy_path = 'data/xas_energy_merged.p'\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "        \n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "data, species_tot = load_data(data_path, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude select samples\n",
    "data_inc = data[data['tag'] == 'INC'].reset_index(drop=True)\n",
    "print('number of inconclusive samples:', len(data_inc))\n",
    "\n",
    "data_weyl = data[data['tag'] == 'WEYL'].reset_index(drop=True)\n",
    "print('number of weyl semimetal samples:', len(data_weyl))\n",
    "\n",
    "data = data[data['tag'].isna()].reset_index(drop=True)\n",
    "print('number of train/valid/test samples:', len(data))\n",
    "\n",
    "species = get_species(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# either compute train/valid/test split or read in computed indices\n",
    "%matplotlib inline\n",
    "split = False\n",
    "\n",
    "if split:\n",
    "    # compute train/valid/test split\n",
    "    idx_train, idx_valid, idx_test = train_valid_test_split(data, species, valid_size=0.1, test_size=0.1, seed=seed)\n",
    "    \n",
    "else:\n",
    "    # load computed train/valid/test split\n",
    "    with open('data/idx_train.txt', 'r') as f:\n",
    "        idx_train = [int(i.split('\\n')[0]) for i in f.readlines()]\n",
    "        \n",
    "    with open('data/idx_valid.txt', 'r') as f:\n",
    "        idx_valid = [int(i.split('\\n')[0]) for i in f.readlines()]\n",
    "        \n",
    "    with open('data/idx_test.txt', 'r') as f:\n",
    "        idx_test = [int(i.split('\\n')[0]) for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding atom type\n",
    "Z = [Element(specie).Z for specie in species_tot]\n",
    "n_type = max(Z)\n",
    "\n",
    "type_encoding = {}\n",
    "for z in range(1, n_type + 1):\n",
    "    specie = Element.from_Z(z)\n",
    "    type_encoding[specie.symbol] = z - 1\n",
    "\n",
    "type_onehot = torch.eye(len(type_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize spectra\n",
    "species_dict = dict(zip(species_tot, [[] for k in range(len(species_tot))]))\n",
    "for entry in data.iloc[idx_train].itertuples():\n",
    "    for specie, xas in entry.spectra_abs.items():\n",
    "        species_dict[specie] += [xas['y'].copy()]\n",
    "        \n",
    "for specie in species_tot:\n",
    "    try: xas = np.stack(species_dict[specie], axis=0)\n",
    "    except:\n",
    "        species_dict[specie] = {'mu': np.nan, 's': np.nan}\n",
    "    else:\n",
    "        if len(species_dict[specie]) > 1:\n",
    "            species_dict[specie] = {'mu': xas.mean(axis=1).mean(), 's': xas.std(axis=1).mean()}\n",
    "        elif len(species_dict[specie]) == 1:\n",
    "            species_dict[specie] = {'mu': xas.mean(), 's': 1.}\n",
    "\n",
    "def standardize_xas(x):\n",
    "    keys = list(x.keys())\n",
    "    y = dict.fromkeys(keys)\n",
    "    for specie in keys:\n",
    "        if np.isnan(species_dict[specie]['mu']):\n",
    "            y[specie] = np.zeros(np.array(x[specie]['y']).shape)\n",
    "        else:\n",
    "            y[specie] = (np.array(x[specie]['y']) - species_dict[specie]['mu'])/species_dict[specie]['s']\n",
    "    return y\n",
    "\n",
    "data['spectra_standard'] = data['spectra_abs'].map(lambda x: standardize_xas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "class DataSet(tg.data.Data):\n",
    "    def __init__(self, x, y, edge_index=None, edge_type=None, z=None):\n",
    "        super(DataSet, self).__init__(x=x, y=y, edge_index=edge_index, edge_type=edge_type, z=z)\n",
    "\n",
    "def build_data(r):\n",
    "    elems = list(r.spectra_standard.keys())\n",
    "    spect = np.stack(list(r.spectra_standard.values()))\n",
    "    x = torch.tensor(spect)\n",
    "    \n",
    "    z = type_onehot[[type_encoding[specie] for specie in elems]]\n",
    "    \n",
    "    y = torch.tensor([r.class_true], dtype=float)  \n",
    "    edge_type = torch.LongTensor([type_encoding[specie] for specie in elems])\n",
    "    return DataSet(x=x, y=y, z=z, edge_type=edge_type)\n",
    "\n",
    "data['data'] = data.apply(build_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dataloaders\n",
    "device = \"cuda:6\"\n",
    "batch_size = 64\n",
    "dataloader_train = DataLoader(data.iloc[idx_train]['data'].tolist(), batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(data.iloc[idx_valid]['data'].tolist(), batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(data.iloc[idx_test]['data'].tolist(), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('number of training batches:', len(dataloader_train))\n",
    "print('number of validation batches:', len(dataloader_valid))\n",
    "print('number of testing batches:', len(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "y_train = data.iloc[idx_train]['data'].map(lambda x: x['y'].item()).tolist()\n",
    "y_valid = data.iloc[idx_valid]['data'].map(lambda x: x['y'].item()).tolist()\n",
    "y_test = data.iloc[idx_test]['data'].map(lambda x: x['y'].item()).tolist()\n",
    "class_weights = []\n",
    "for k in range(num_classes):\n",
    "    class_weights += [y_train.count(k)]\n",
    "    print(\"class:\", k)\n",
    "    print(\"total:\", (y_train + y_valid + y_test).count(k))\n",
    "    print(\"train:\", class_weights[k])\n",
    "    print(\"valid:\", y_valid.count(k))\n",
    "    print(\"test:\", y_test.count(k), \"\\n\")\n",
    "class_weights = [1./k for k in class_weights]\n",
    "cw_mean = np.mean(class_weights)\n",
    "class_weights = torch.tensor([k/cw_mean for k in class_weights])\n",
    "print(\"class weights:\", class_weights)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self, length, embed_dim, hidden_dim, num_dense, num_classes, class_weights=None, slope=0.01, drop=0,\n",
    "                 scale=2, reduce=True, type_only=False):\n",
    "        super(GraphNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.class_weights = class_weights\n",
    "        self.reduce = reduce\n",
    "        self.type_only = type_only\n",
    "        \n",
    "        try: len(class_weights)\n",
    "        except: self.pos_weight = None\n",
    "        else: self.pos_weight = self.class_weights[1]/self.class_weights[0]\n",
    "        \n",
    "        # atom type embedding\n",
    "        self.embed = nn.Linear(in_features=n_type, out_features=embed_dim)\n",
    "        \n",
    "        if not self.type_only:\n",
    "            # fully-connected\n",
    "            self.fc = self.linear_block_chain(length, hidden_dim, num_dense-1, slope, drop, scale)\n",
    "        \n",
    "        # fully-connected\n",
    "        self.fcc = self.linear_block_chain(hidden_dim*embed_dim, hidden_dim, num_dense-1, slope, drop, scale)\n",
    "        \n",
    "        # symmetry\n",
    "        self.sc = nn.Linear(in_features=hidden_dim, out_features=7)\n",
    "        \n",
    "        # classifier\n",
    "        if num_classes == 2:\n",
    "            self.classifier = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n",
    "    \n",
    "    def linear_block_chain(self, in_features, hidden_dim, num_dense, slope, drop=0, scale=2):\n",
    "        \n",
    "        if in_features < hidden_dim: shift = -(num_dense - 1)\n",
    "        else: shift = num_dense - 1\n",
    "            \n",
    "        out_features = (np.logspace(np.log10(hidden_dim)/np.log10(scale),\n",
    "                                    np.log10(hidden_dim)/np.log10(scale) + shift,\n",
    "                                    num_dense, base=scale)[::-1]).astype(int)\n",
    "        \n",
    "        modules = []\n",
    "        for i in range(num_dense):\n",
    "            modules.append(\n",
    "                nn.Sequential(*self.linear_block(in_features, out_features[i], slope, drop)))\n",
    "            in_features = out_features[i]\n",
    "        \n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "    def linear_block(self, in_features, out_features, slope, drop=0):\n",
    "        modules = [nn.Linear(in_features=in_features, out_features=out_features)]\n",
    "        \n",
    "        if drop: modules.append(nn.Dropout(p=drop))\n",
    "        modules.append(nn.LeakyReLU(negative_slope=slope))\n",
    "        \n",
    "        return modules\n",
    "        \n",
    "    def loss_function(self, y_pred, y_true):\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)(y_pred, y_true)\n",
    "    \n",
    "    def get_score(self, y):\n",
    "        return torch.sigmoid(y)\n",
    "        \n",
    "    def get_class(self, y, th=0.5):\n",
    "        return (self.get_score(y) >= th).float()\n",
    "\n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        y_pred_class = self.get_class(y_pred)\n",
    "        y_corr = (y_pred_class == y_true).float()\n",
    "        acc = y_corr.sum()/len(y_corr)*100.\n",
    "        return acc\n",
    "    \n",
    "    def process(self, data):\n",
    "        # embed z\n",
    "        z = self.embed(data.z)\n",
    "        \n",
    "        # spectrum transformation\n",
    "        if self.type_only:\n",
    "            x = torch.ones((z.size()[0], self.hidden_dim), device=z.device)\n",
    "        else:\n",
    "            x = self.fc(data.x)\n",
    "        \n",
    "        # outer product with atom type\n",
    "        x = torch.einsum('bi,bj->bij', (z, x))\n",
    "        \n",
    "        # aggregate spectra\n",
    "        if self.reduce:\n",
    "            x = scatter(x, data.batch, dim=0, reduce='sum')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # process spectra\n",
    "        x = self.process(data)\n",
    "        \n",
    "        # unravel channels\n",
    "        x = x.flatten(start_dim=1)\n",
    "        \n",
    "        # feed-forward\n",
    "        x = self.fcc(x)\n",
    "        \n",
    "        # classification\n",
    "        return self.classifier(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "length = 200\n",
    "width = 128\n",
    "embed_dim = n_type\n",
    "hidden_dim = 64\n",
    "num_dense = 3\n",
    "slope = 0.01\n",
    "drop = 0.5\n",
    "scale_dense = 4\n",
    "reduce = True\n",
    "type_only = False\n",
    "\n",
    "model = GraphNN(length, embed_dim, hidden_dim, num_dense, num_classes, class_weights, slope, drop,\n",
    "                scale_dense, reduce, type_only).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation functions\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_cumulative = 0.\n",
    "    acc_cumulative = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            y_pred = model(d)\n",
    "            loss = model.loss_function(y_pred, d.y).cpu()\n",
    "                \n",
    "            acc = model.accuracy(y_pred, d.y).cpu()\n",
    "            loss_cumulative += loss.detach().item()\n",
    "            acc_cumulative += acc.detach().item()\n",
    "        \n",
    "    return loss_cumulative/len(dataloader), acc_cumulative/len(dataloader)\n",
    "\n",
    "def train(model, optimizer, dataloader_train, dataloader_valid, dynamics, step_0, max_iter=1000, device=\"cpu\"):\n",
    "    checkpoint = 5\n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        \n",
    "        loss_cumulative = 0.\n",
    "        acc_cumulative = 0.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for j, d in enumerate(dataloader_train):\n",
    "            d.to(device)\n",
    "            y_pred = model(d)\n",
    "            loss = model.loss_function(y_pred, d.y).cpu()\n",
    "                \n",
    "            acc = model.accuracy(y_pred, d.y).cpu()\n",
    "            \n",
    "            print(f\"Iteration {step+1:5d}    batch {j+1:5d} / {len(dataloader_train):5d}   \" +\n",
    "                  f\"batch loss = {loss.data:.7e}\", end=\"\\r\", flush=True)\n",
    "            loss_cumulative += loss.detach().item()\n",
    "            acc_cumulative += acc.detach().item()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        wall = end_time - start_time\n",
    "        \n",
    "        if (step+1)%checkpoint == 0:\n",
    "            print(f\"Iteration {step+1:5d}    batch {j+1:5d} / {len(dataloader_train):5d}   \" +\n",
    "                  f\"epoch loss = {loss_cumulative / len(dataloader_train):.7e}\")\n",
    "\n",
    "            valid_avg_loss = evaluate(model, dataloader_valid, device)\n",
    "            train_avg_loss = evaluate(model, dataloader_train, device)\n",
    "\n",
    "            dynamics.append({\n",
    "                'step': step + step_0,\n",
    "                'wall': wall,\n",
    "                'batch': {\n",
    "                    'loss': loss.item(),\n",
    "                    'accuracy': acc.item(),\n",
    "                },\n",
    "                'valid': {\n",
    "                    'loss': valid_avg_loss[0],\n",
    "                    'accuracy': valid_avg_loss[1],\n",
    "                },\n",
    "                 'train': {\n",
    "                     'loss': train_avg_loss[0],\n",
    "                     'accuracy': train_avg_loss[1],\n",
    "                 },\n",
    "            })\n",
    "\n",
    "            yield {\n",
    "                'dynamics': dynamics,\n",
    "                'state': model.state_dict(),\n",
    "                'optimizer': opt.state_dict()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = 'models/nn_onehot.torch'\n",
    "model_path = 'models/nn.torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "resume_fit = False\n",
    "if resume_fit:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)['state'])\n",
    "    opt.load_state_dict(torch.load(model_path, map_location=device)['optimizer'])\n",
    "    dynamics = torch.load(model_path, map_location=device)['dynamics']\n",
    "    step_0 = dynamics[-1]['step'] + 1\n",
    "else:\n",
    "    dynamics = []\n",
    "    step_0 = 0\n",
    "    \n",
    "for results in train(model, opt, dataloader_train, dataloader_valid, dynamics, step_0, max_iter=80, device=device):\n",
    "    with open(model_path, 'wb') as f:\n",
    "        torch.save(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "%matplotlib inline\n",
    "saved = torch.load(model_path, map_location=device)\n",
    "steps = [d['step'] + 1 for d in saved['dynamics']]\n",
    "loss_train = [d['train']['loss'] for d in saved['dynamics']]\n",
    "loss_valid = [d['valid']['loss'] for d in saved['dynamics']]\n",
    "acc_train = [d['train']['accuracy'] for d in saved['dynamics']]\n",
    "acc_valid = [d['valid']['accuracy'] for d in saved['dynamics']]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(11,4.5))\n",
    "prop.set_size(16)\n",
    "\n",
    "ax[0].axhline(np.mean(loss_valid[-10:]), color='black', ls='dashed')\n",
    "ax[0].plot(steps, loss_train, lw=2, label=\"Train\", color='#6A71B4')\n",
    "ax[0].plot(steps, loss_valid, lw=2, label=\"Valid.\", color='#BB6D89')\n",
    "\n",
    "ax[1].axhline(np.mean(acc_valid[-10:]), color='black', ls='dashed')\n",
    "ax[1].plot(steps, acc_train, lw=2, label=\"Train\", color='#6A71B4')\n",
    "ax[1].plot(steps, acc_valid, lw=2, label=\"Valid\", color='#BB6D89')\n",
    "\n",
    "format_axis(ax[0], 'epochs', 'loss', prop, legend=True)\n",
    "format_axis(ax[1], 'epochs', 'accuracy', prop)\n",
    "fig.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model for prediction\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, th=None):\n",
    "    # evaluate trained model over all datasets\n",
    "    true = np.zeros((len(dataloader.dataset),))\n",
    "    pred = np.zeros((len(dataloader.dataset),))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        k = 0\n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            y_pred = model(d)\n",
    "            true[k:k + len(y_pred)] = d.y.cpu().numpy()\n",
    "            \n",
    "            if th:\n",
    "                # apply threshold to predicted class\n",
    "                pred[k:k + len(y_pred)] = model.get_class(y_pred, th=th).cpu().numpy()\n",
    "            else:\n",
    "                # get sigmoid-activated class prediction without applied threshold\n",
    "                pred[k:k + len(y_pred)] = list(torch.sigmoid(y_pred).cpu().numpy())\n",
    "            k += len(y_pred)\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance on all datasets\n",
    "true_train, pred_train = evaluate_model(model, dataloader_train, device)\n",
    "true_valid, pred_valid = evaluate_model(model, dataloader_valid, device)\n",
    "true_test, pred_test = evaluate_model(model, dataloader_test, device)\n",
    "\n",
    "fpr, tpr, roc_auc, th = [], [], [], []\n",
    "for y_true, y_pred in zip([true_train, true_valid, true_test], [pred_train, pred_valid, pred_test]):\n",
    "    fpri, tpri, auci, thi = get_roc(y_pred, y_true)\n",
    "    fpr += [fpri]\n",
    "    tpr += [tpri]\n",
    "    roc_auc += [auci]\n",
    "    th += [thi]\n",
    "\n",
    "# get optimal threshold using validation set\n",
    "tpr0, fpr0, th0 = get_optimal_threshold(fpr[1], tpr[1], th[1])\n",
    "\n",
    "# plot operating curves\n",
    "fig, ax = plt.subplots(1,1, figsize=(4.7,4.3))\n",
    "prop.set_size(16)\n",
    "plot_roc(ax, fpr, tpr, roc_auc)\n",
    "ax.legend(loc='lower right', prop=prop, frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with optimal threshold\n",
    "true_train, pred_train = evaluate_model(model, dataloader_train, device, th=th0)\n",
    "true_valid, pred_valid = evaluate_model(model, dataloader_valid, device, th=th0)\n",
    "true_test, pred_test = evaluate_model(model, dataloader_test, device, th=th0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "fig, ax = plt.subplots(1,3, figsize=(8,3))\n",
    "print('accuracy (unweighted):', np.count_nonzero(true_train == pred_train)/len(true_train))\n",
    "plot_precision_recall_fscore(ax, true_train, pred_train, num_classes, prop)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "fig, ax = plt.subplots(1,3, figsize=(8,3))\n",
    "print('accuracy (unweighted):', np.count_nonzero(true_valid == pred_valid)/len(true_valid))\n",
    "plot_precision_recall_fscore(ax, true_valid, pred_valid, num_classes, prop)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "fig, ax = plt.subplots(1,3, figsize=(8,3))\n",
    "print('accuracy (unweighted):', np.count_nonzero(true_test == pred_test)/len(true_test))\n",
    "plot_precision_recall_fscore(ax, true_test, pred_test, num_classes, prop)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element specific results for testing data\n",
    "data_test = data.iloc[idx_test].copy()\n",
    "data_test['class_pred'] = pred_test.astype(int)\n",
    "ptable_test = get_element_results(data_test, species)\n",
    "\n",
    "# save periodic table of element results\n",
    "cnorm = (0, 1)\n",
    "for attribute in ['recall_topo', 'precision_topo', 'f1_topo', 'recall_triv', 'precision_triv', 'f1_triv']:\n",
    "    outfile = 'images/table_' + attribute\n",
    "    periodic_plot(ptable_test, attribute=attribute, colorby='attribute', cmap=cmap, cnorm=cnorm, output=outfile)\n",
    "\n",
    "# save periodic table colorbar\n",
    "for attribute in ['recall', 'precision', '$F_1$']:\n",
    "    sm = mpl.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=cnorm[0], vmax=cnorm[1]))    \n",
    "    sm.set_array([])\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6,0.4))\n",
    "    cbar = plt.colorbar(sm, cax=ax, orientation='horizontal')\n",
    "    fontsize = 16\n",
    "    format_axis(cbar.ax, xlabel='', ylabel='', prop=prop)\n",
    "    cbar.ax.tick_params(labelsize=fontsize)\n",
    "    ax.set_xlabel(attribute, fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
